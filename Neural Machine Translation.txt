Neural Machine Translation by Jointly Learning to Align and Translate
Unlike the traditional statistical machine translation,
the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance.

The models proposed recently for neural machine translation often belong to a family of encoder-decoders
and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation.

it does not attempt to encode a whole input sentence into a single fixed-length vector. Instead, it en
codes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively
while decoding the translation.  

In
 neural machine translation, we fit a parameterized model to maximize the conditional probability
 of sentence pairs using a parallel training corpus. Once the conditional distribution is learned by a
 translation model, given a source sentence a corresponding translation can be generated by searching
 for the sentence that maximizes the conditional probability.

